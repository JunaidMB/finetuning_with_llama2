{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YbIBXcexEdo"
      },
      "source": [
        "# Finetune Mistral-7b model on the Guanaco Dataset - Using Supervised Fine Tuning Trainer\n",
        "\n",
        "Task Description: Causal Language Modelling (CLM) is text generation. Given a prompt/source sequence, CLM will generate words to continue the source sequence.\n",
        "\n",
        "Original Tutorial: https://colab.research.google.com/drive/1DNenc5BpdqaS10prtklYyIe9qW_7gUnb?usp=sharing#scrollTo=ZwXZbQ2dSwzI\n",
        "\n",
        "Credit to **Younes Belkada** for the original notebook\n",
        "\n",
        "The Guanaco dataset is a clean subset of the OpenAssistant dataset adapted to train general purpose chatbots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfiVGwyMxKEE",
        "outputId": "de5f6900-28b9-43ec-e050-e40940417b26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U trl accelerate git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q datasets bitsandbytes evaluate accelerate bitsandbytes loralib  python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocHWFEgMxUmn"
      },
      "source": [
        "# Load Guanaco dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geYs0U6JxRxV",
        "outputId": "72eaed5c-3702-4c66-cab0-24db054e2b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "guanaco = load_dataset(\n",
        "    \"timdettmers/openassistant-guanaco\",\n",
        "    split = \"train\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "guanaco"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEdiyaL5RFeJ",
        "outputId": "eb44b20f-098e-403c-cd44-cca6be911f0b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 9846\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ZHO-hefIxqYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b26b087-56c6-4bbb-c7bd-23b63272c978"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 7876\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1970\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Split the dataset into a train and test set\n",
        "guanaco = guanaco.train_test_split(test_size=0.2)\n",
        "guanaco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtcEya7Kx4-x",
        "outputId": "a1c2a68f-bbb0-4fa1-92e0-6b1d11d3111e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '### Human: 安装并使用诸如openpyxl的库，使用其读取方法即可### Assistant: '\n",
            "         '好的，以下是使用openpyxl库读取Excel文件的示例代码：\\n'\n",
            "         '\\n'\n",
            "         '先安装openpyxl库，可以在命令行中使用如下命令：\\n'\n",
            "         '```\\n'\n",
            "         'pip install openpyxl\\n'\n",
            "         '```\\n'\n",
            "         '然后可以使用如下代码读取Excel文件：\\n'\n",
            "         '```\\n'\n",
            "         'import openpyxl\\n'\n",
            "         '\\n'\n",
            "         '# 打开Excel文件\\n'\n",
            "         \"workbook = openpyxl.load_workbook('example.xlsx')\\n\"\n",
            "         '\\n'\n",
            "         '# 选择表格\\n'\n",
            "         \"worksheet = workbook['Sheet1']\\n\"\n",
            "         '\\n'\n",
            "         '# 读取单元格的值\\n'\n",
            "         'cell_value = worksheet.cell(row=1, column=1).value\\n'\n",
            "         '\\n'\n",
            "         '# 打印单元格的值\\n'\n",
            "         'print(cell_value)\\n'\n",
            "         '```\\n'\n",
            "         '在上面的代码中，我们首先使用load_workbook()方法打开了一个Excel文件，并选择了一个表格。然后使用cell()方法读取第1行第1列的单元格的值，并将它打印出来。'}\n"
          ]
        }
      ],
      "source": [
        "# Look at the data\n",
        "from pprint import pprint\n",
        "pprint(guanaco['train'][1000])\n",
        "\n",
        "# The text column is our model input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Tokenizer"
      ],
      "metadata": {
        "id": "RD7xcqDJJmgD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbtvEgvlyEbI",
        "outputId": "75d3e53c-1524-4df1-c8ed-c3ff7650f87b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# Load Tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"ybelkada/Mistral-7B-v0.1-bf16-sharded\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Add special tokens\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZMOI3k8ZBnM"
      },
      "source": [
        "# Load a Model with BitsandBytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DoJOpLShY7Yu"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import torch\n",
        "\n",
        "# Create a config corresponding to the PEFT method\n",
        "peft_config = LoraConfig(\n",
        "    task_type = TaskType.CAUSAL_LM,\n",
        "    inference_mode = False,\n",
        "    r=64,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1\n",
        ")\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "  # Load Model in 4bit precision\n",
        "   load_in_4bit=True,\n",
        "  # use normalized float 4 (default)\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "  # uses a second quantization after the first one to save an additional 0.4 bits per parameter\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "  # Format in which computations will occur\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "87ada316346244fb85ad0b4fbc019f3f",
            "b1877298efd6476eb2514b3a93ffe7b1",
            "57fb32ca768e4b2e9fbcfd5ddaa2a890",
            "e6ae560fe06f485baf76dde197e9406e",
            "de13b22d62504605a830916bd6a9f362",
            "b0a2413cf6344d83bcd532b133afa72a",
            "f025a327b58844bcaf502b5ee83b474c",
            "ef6f27eb32a24b73971712dbb792865d",
            "ee6d2bb3d4af4eeab804c032d7ccc13c",
            "e99c9842838942d7b0541c2d95e1e99e",
            "6a12dd02f2b34565ad131f17331bbc30"
          ]
        },
        "id": "gLpPmdwwaF-_",
        "outputId": "2ca94d9e-e9c1-45db-8af6-d77939d9364b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87ada316346244fb85ad0b4fbc019f3f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Wrap base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    checkpoint,\n",
        "    quantization_config = nf4_config,\n",
        "    #device_map=\"auto\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqFuv8u5aiyf",
        "outputId": "5c734249-3bb2-43d1-92ed-d192a7cf6296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): MistralRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm()\n",
            "        (post_attention_layernorm): MistralRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjcuQs822Wkv"
      },
      "source": [
        "# Train using the SFTTrainer API\n",
        "The main training steps are:\n",
        "\n",
        "1. Define training hyperparameters using a model specific TrainingArguments function. At the end of each epoch, the Trainer will evaluate the defined loss metric and save the training checkpoint.\n",
        "\n",
        "2. Pass the training arguments to a Trainer function alongside the model, dataset, tokenizer, data collator.\n",
        "\n",
        "3. Call train() to finetune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nNB5hINQ3MLe"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"./guanaco_clm\"\n",
        "per_device_train_batch_size = 4\n",
        "gradient_accumulation_steps = 4\n",
        "optim = \"paged_adamw_32bit\"\n",
        "save_steps = 10\n",
        "logging_steps = 10\n",
        "learning_rate = 2e-4\n",
        "max_grad_norm = 0.3\n",
        "max_steps = 10\n",
        "warmup_ratio = 0.03\n",
        "lr_scheduler_type = \"constant\"\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=True,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=False,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    gradient_checkpointing=True,\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "MxKtf1FOcsdV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "max_seq_length = 512\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=guanaco['train'],\n",
        "    eval_dataset=guanaco['test'],\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=True,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InOUSMiycx8e",
        "outputId": "c57d1e68-6abe-4cfc-937c-baa6e358aabf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:214: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:234: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upcast the layer norms in float32 for more stable training\n",
        "for name, module in trainer.model.named_modules():\n",
        "    if \"norm\" in name:\n",
        "        module = module.to('cuda')"
      ],
      "metadata": {
        "id": "tsjI7HeHdj3_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "5ivZBznDO_mF",
        "outputId": "8e9c1fa9-51c6-4d3c-b67a-46858656d184"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 01:49, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.238300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=1.23828125, metrics={'train_runtime': 122.7214, 'train_samples_per_second': 1.304, 'train_steps_per_second': 0.081, 'total_flos': 3569248685260800.0, 'train_loss': 1.23828125, 'epoch': 0.02})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xbqxh3bN4TeR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cf0d08f8-79e1-4855-d514-f5465194059d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='220' max='247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [220/247 04:59 < 00:36, 0.73 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 3.37\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the fine tuned model and obtain the perplexity score\n",
        "import math\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nm1XQNnI4g6I"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"guanaco_causal_modell\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MYYZQz3E7oKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb9ac95-bea9-4d80-d277-44cf2ee96b09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('guanaco_causal_modell/tokenizer_config.json',\n",
              " 'guanaco_causal_modell/special_tokens_map.json',\n",
              " 'guanaco_causal_modell/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# In this case, the tokenizer was not saved automatically, save it manually in the model folder for inference\n",
        "#tokenizer.save_pretrained(\"guanaco_causal_modell\", legacy_format=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXjM0h0C4s_F"
      },
      "source": [
        "# Inference\n",
        "\n",
        "Use model for inference using a pipeline wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "y0UngNKXdD6q"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# Warning: Saving and Reloading will cause a memory shortage. The rest of the notebook uses the in memory trainer model.\n",
        "peft_model_id = \"guanaco_causal_modell\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"ybelkada/Mistral-7B-v0.1-bf16-sharded\",\n",
        "    quantization_config = nf4_config,\n",
        "    #device_map=\"auto\",\n",
        "    )\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYHpISpjdDop"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PTQDfxBbdDU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa18d828-c592-48a2-b6cc-1fbad103465a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32000, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): Linear4bit(\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear4bit(\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear4bit(\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): Linear4bit(\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              )\n",
              "              (up_proj): Linear4bit(\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              )\n",
              "              (down_proj): Linear4bit(\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=64, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm()\n",
              "            (post_attention_layernorm): MistralRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "device = \"cuda\"\n",
        "model = trainer.model\n",
        "model = model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "VUvQcBvFgJk4"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "### Human: Can you please write a short paragraph about why philosophy is an important subject for study. ###\"\n",
        "Assistant:\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HY2_zVlo4xy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8dec076-9829-4b6c-824c-7c97716fafc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    1, 28705,    13, 27332, 10649, 28747,  2418,   368,  4665,  3324,\n",
            "           264,  2485, 18438,   684,  2079, 13795,   349,   396,  2278,  3817,\n",
            "           354,  3881, 28723,   774, 28739,    13,  7226, 11143, 28747,    13]])\n"
          ]
        }
      ],
      "source": [
        "# Inference Pipeline using Pytorch\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "uM7bBeYChV8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f5a6cb-ec9c-4169-d18b-32a8afd68903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Generate method is used to generate text\n",
        "  outputs = model.generate(\n",
        "      input_ids=inputs.to(device),\n",
        "      max_new_tokens=100,\n",
        "      do_sample=True,\n",
        "      top_k=50,\n",
        "      top_p=0.95\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JtktWMuGhgIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4703353a-a342-49da-e77a-58d5dce0490b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n### Human: Can you please write a short paragraph about why philosophy is an important subject for study. ###\"\\nAssistant:\\nAbsolutely! Philosophy is an important subject because it helps us to understand the world, our place in it, and our potential to live fulfilled and meaningful lives. It provides a framework for critical thinking, helps us to ask important questions about the nature of reality, and develops our ability to engage in deep and meaningful conversations with others.\\n\\nAdditionally, philosophy is a critical component of a liberal arts education, and can help us to develop a well-rounded education and a broader']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Decode the generated token ids back into text\n",
        "import pprint\n",
        "import numpy as np\n",
        "\n",
        "decoded_output = tokenizer.batch_decode(outputs.detach().cpu().numpy(),\n",
        "                                     skip_special_tokens=True,\n",
        "                                     )\n",
        "decoded_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8brWInxW486o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e94cae9-7344-4775-aa0e-161cdd07b8ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Human: Can you please write a short paragraph about why philosophy is an important subject for study. ###\"\n",
            "Assistant:\n",
            "Absolutely! Philosophy is an important subject because it helps us to understand the world, our place in it, and our potential to live fulfilled and meaningful lives. It provides a framework for critical thinking, helps us to ask important questions about the nature of reality, and develops our ability to engage in deep and meaningful conversations with others.\n",
            "\n",
            "Additionally, philosophy is a critical component of a liberal arts education, and can help us to develop a well-rounded education and a broader\n"
          ]
        }
      ],
      "source": [
        "print(\"\".join(decoded_output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvRw8bBb5qA9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "87ada316346244fb85ad0b4fbc019f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1877298efd6476eb2514b3a93ffe7b1",
              "IPY_MODEL_57fb32ca768e4b2e9fbcfd5ddaa2a890",
              "IPY_MODEL_e6ae560fe06f485baf76dde197e9406e"
            ],
            "layout": "IPY_MODEL_de13b22d62504605a830916bd6a9f362"
          }
        },
        "b1877298efd6476eb2514b3a93ffe7b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0a2413cf6344d83bcd532b133afa72a",
            "placeholder": "​",
            "style": "IPY_MODEL_f025a327b58844bcaf502b5ee83b474c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "57fb32ca768e4b2e9fbcfd5ddaa2a890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef6f27eb32a24b73971712dbb792865d",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee6d2bb3d4af4eeab804c032d7ccc13c",
            "value": 8
          }
        },
        "e6ae560fe06f485baf76dde197e9406e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e99c9842838942d7b0541c2d95e1e99e",
            "placeholder": "​",
            "style": "IPY_MODEL_6a12dd02f2b34565ad131f17331bbc30",
            "value": " 8/8 [01:27&lt;00:00,  9.64s/it]"
          }
        },
        "de13b22d62504605a830916bd6a9f362": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0a2413cf6344d83bcd532b133afa72a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f025a327b58844bcaf502b5ee83b474c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef6f27eb32a24b73971712dbb792865d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee6d2bb3d4af4eeab804c032d7ccc13c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e99c9842838942d7b0541c2d95e1e99e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a12dd02f2b34565ad131f17331bbc30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}